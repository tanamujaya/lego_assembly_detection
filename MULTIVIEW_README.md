# LEGO Assembly Error Detection - Multi-View System

## ğŸ¯ Complete System with Multi-View Support

This is the complete LEGO Assembly Error Detection System with **multi-view inspection capabilities** added.

---

## ğŸ“¦ What's Included

### Core Detection System (Original):
- `main.py` - Main CLI interface
- `config.py` - System configuration
- `data_preparation.py` - Dataset preparation utilities
- `model_trainer.py` - Training pipeline
- `model_evaluator.py` - Evaluation metrics
- `inference.py` - Single-view inference
- `training_pipeline.py` - K-fold cross-validation
- `evaluation.py` - Performance visualization
- `requirements.txt` - Python dependencies

### Multi-View Extensions (NEW):
- `multiview_config.py` - Multi-view configuration and helpers
- `multiview_inference.py` - 4-view inspection and evaluation
- `MULTIVIEW_GUIDE.py` - Comprehensive implementation guide
- `QUICK_REFERENCE.md` - Quick command reference

### Documentation:
- `README.md` - Original system documentation
- `START_HERE.md` - Quick start guide
- `DEPLOYMENT_GUIDE.md` - Raspberry Pi deployment
- Various architecture and usage documentation

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Verify installation
python main.py --help
```

### 2. Prepare Your Multi-View Dataset

Your dataset should be structured like:
```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image_0000_0.png     # Model 0, View 0Â°
â”‚   â”œâ”€â”€ image_0000_90.png    # Model 0, View 90Â°
â”‚   â”œâ”€â”€ image_0000_180.png   # Model 0, View 180Â°
â”‚   â”œâ”€â”€ image_0000_270.png   # Model 0, View 270Â°
â”‚   â”œâ”€â”€ image_0001_0.png     # Different images, same view
â”‚   â””â”€â”€ ...
â””â”€â”€ labels/
    â”œâ”€â”€ image_0000_0.txt     # YOLO format: class_id x y w h
    â”œâ”€â”€ image_0000_90.txt
    â””â”€â”€ ...
```

### 3. Train Model (Standard Training - No Changes)

```bash
# K-Fold Cross-Validation (Recommended)
python main.py train \
  --mode kfold \
  --dataset ./data/your_dataset \
  --model yolov8n \
  --k 10 \
  --epochs 100

# Or Standard Train/Val/Test Split
python main.py train \
  --mode standard \
  --dataset ./data/your_dataset \
  --model yolov8n \
  --epochs 100
```

### 4. Multi-View Inference (NEW)

#### Single Model Inspection:
```python
from multiview_inference import MultiViewInspector

# Initialize inspector
inspector = MultiViewInspector(
    model_path='./models/yolov8n_train/weights/best.pt',
    confidence_threshold=0.5,
    decision_strategy='any_error',  # Recommended for safety
    save_results=True
)

# Inspect a model (4 views)
image_paths = {
    0:   './test/image_0000_0.png',
    90:  './test/image_0000_90.png',
    180: './test/image_0000_180.png',
    270: './test/image_0000_270.png'
}

result = inspector.inspect_assembly_multiview(
    image_paths=image_paths,
    model_id='test_model_0000'
)

print(f"Final Decision: {result['final_decision_label']}")
print(f"Confidence: {result['confidence']:.3f}")
```

#### Batch Evaluation:
```python
from multiview_inference import MultiViewBatchEvaluator

evaluator = MultiViewBatchEvaluator(
    model_path='./models/yolov8n_train/weights/best.pt',
    decision_strategy='any_error'
)

results = evaluator.evaluate_test_set(
    images_dir='./data/test/images',
    labels_dir='./data/test/labels',
    output_path='./multiview_results.json'
)

print(f"Model-Level Accuracy: {results['model_accuracy']:.2%}")
print(f"Per-View Accuracy: {results['per_view_accuracy']}")
```

---

## ğŸ“ How Multi-View Works

### Training Phase:
- **Each view is trained independently** as a separate sample
- Model learns: "Is THIS specific view correct (0) or incorrect (1)?"
- No special handling needed - use standard training pipeline

### Deployment Phase:
- **Capture/load 4 views** of the same model (0Â°, 90Â°, 180Â°, 270Â°)
- **Run inference** on each view independently
- **Aggregate predictions** using decision strategy
- **Final decision**: CORRECT âœ… or INCORRECT âŒ

### Decision Strategies:

| Strategy | Logic | Use Case |
|----------|-------|----------|
| `any_error` â­ | If ANY view shows error â†’ INCORRECT | High safety (recommended) |
| `majority_vote` | If >50% views show error â†’ INCORRECT | Balanced approach |
| `all_error` | If ALL views show error â†’ INCORRECT | Minimize false positives |

**Recommendation**: Use `any_error` for assembly verification

---

## ğŸ“Š Expected Performance

### Single-View Performance:
- Per-view accuracy: 85-90%
- Good but may miss errors visible only from certain angles

### Multi-View Performance:
- Model-level accuracy: **92-95%**
- **5-10% improvement** over single best view
- Much more robust and reliable

### Why Multi-View is Better:
- âœ… Catches errors invisible from single angle
- âœ… More robust to lighting variations
- âœ… Provides redundancy and reliability
- âœ… Diagnostic info (which views detected error)

---

## ğŸ¤– Raspberry Pi Deployment

### Hardware Setup:
- Raspberry Pi 4B (4GB or 8GB RAM)
- Pi Camera Module or USB Camera
- Stepper motor + turntable (for automated rotation)
- Optional: GPIO connections for motor control

### Production System Example:
```python
from multiview_inference import MultiViewInspector
import cv2

# Initialize
inspector = MultiViewInspector(
    model_path='./best.pt',
    decision_strategy='any_error'
)

# Capture 4 views (with turntable rotation)
def capture_all_views():
    camera = cv2.VideoCapture(0)
    views = {}
    
    for angle in [0, 90, 180, 270]:
        # Rotate turntable to angle
        rotate_turntable(angle)
        time.sleep(0.5)  # Wait for stabilization
        
        # Capture image
        ret, frame = camera.read()
        temp_path = f'./temp_view_{angle}.png'
        cv2.imwrite(temp_path, frame)
        views[angle] = temp_path
    
    camera.release()
    return views

# Inspect
image_paths = capture_all_views()
result = inspector.inspect_assembly_multiview(image_paths)

if result['final_decision_label'] == 'CORRECT':
    print("âœ… PASS - Ship to customer")
else:
    print("âŒ FAIL - Error detected")
```

See `MULTIVIEW_GUIDE.py` for complete Raspberry Pi implementation with turntable control.

---

## ğŸ“ File Structure

```
lego_assembly_detection/
â”œâ”€â”€ Core System
â”‚   â”œâ”€â”€ main.py                    # CLI interface
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â”œâ”€â”€ data_preparation.py        # Dataset utilities
â”‚   â”œâ”€â”€ model_trainer.py           # Training
â”‚   â”œâ”€â”€ inference.py               # Single-view inference
â”‚   â”œâ”€â”€ evaluation.py              # Metrics
â”‚   â””â”€â”€ requirements.txt           # Dependencies
â”‚
â”œâ”€â”€ Multi-View Extensions (NEW)
â”‚   â”œâ”€â”€ multiview_config.py        # Multi-view configuration
â”‚   â”œâ”€â”€ multiview_inference.py     # 4-view inspection
â”‚   â”œâ”€â”€ MULTIVIEW_GUIDE.py         # Implementation guide
â”‚   â””â”€â”€ QUICK_REFERENCE.md         # Quick reference
â”‚
â””â”€â”€ Documentation
    â”œâ”€â”€ README.md                   # Original system docs
    â”œâ”€â”€ MULTIVIEW_README.md         # This file
    â”œâ”€â”€ START_HERE.md               # Quick start
    â””â”€â”€ DEPLOYMENT_GUIDE.md         # RPi deployment
```

---

## ğŸ”§ Configuration

### Image Resolution (Recommended):
```python
# In config.py or when training
DATASET_CONFIG = {
    'image_size': (320, 320),  # Fast on RPi, good accuracy
}
```

### Model Selection:
```python
MODEL_CONFIG = {
    'variant': 'yolov8n',  # Fastest for Raspberry Pi
    # Alternatives: 'yolov8s' (more accurate, slower)
}
```

### Multi-View Settings:
```python
# In multiview_config.py
MULTIVIEW_CONFIG = {
    'decision_strategy': 'any_error',  # Recommended
    'confidence_threshold': 0.5,
    'view_angles': [0, 90, 180, 270]
}
```

---

## ğŸ“š Documentation Guide

1. **Start Here**: `START_HERE.md` - System overview
2. **Training**: `README.md` - Original training documentation
3. **Multi-View**: `MULTIVIEW_GUIDE.py` - Complete multi-view guide
4. **Quick Commands**: `QUICK_REFERENCE.md` - Command reference
5. **Deployment**: `DEPLOYMENT_GUIDE.md` - Raspberry Pi setup

---

## ğŸ†˜ Troubleshooting

### Training Issues:
- **Out of memory**: Reduce batch size (`--batch-size 8`)
- **Slow training**: Use GPU (Google Colab free T4)
- **Low accuracy**: Increase epochs, check data quality

### Multi-View Issues:
- **One view poor**: Check lighting, camera position
- **Too many false positives**: Try `majority_vote` strategy
- **Slow on RPi**: Use `yolov8n`, reduce resolution to 224Ã—224

### Deployment Issues:
- **Model won't load**: Check file path, model compatibility
- **Camera not working**: Check camera is enabled in raspi-config
- **Turntable not rotating**: Verify GPIO connections, power supply

---

## ğŸ’¡ Best Practices

1. **Training**:
   - Use 320Ã—320 resolution for good balance of speed/accuracy
   - Train with data augmentation enabled
   - Use K-fold cross-validation for robust evaluation

2. **Deployment**:
   - Use `any_error` strategy for safety-critical applications
   - Save inspection results for later analysis
   - Test on static images before adding hardware

3. **Performance**:
   - Use YOLOv8n for Raspberry Pi (fastest)
   - Enable FP16 for faster inference
   - Process views in parallel if memory allows

---

## ğŸ“ˆ Performance Expectations

### Training Time (100 epochs):
- Google Colab (T4 GPU): 4-6 hours
- Local GPU (RTX 3080): 2-3 hours
- CPU: Not recommended (45+ hours)

### Inference Time (4 views):
- Raspberry Pi 4B: 2-4 seconds per model
- YOLOv8n: ~0.5-1s per view
- YOLOv8s: ~1-2s per view

### Accuracy:
- Single-view: 85-90%
- Multi-view: 92-95%
- Improvement: 5-10%

---

## ğŸ¯ Quick Workflow

```bash
# 1. Install
pip install -r requirements.txt

# 2. Prepare dataset (already done if renamed)
# Your dataset should have: image_NNNN_AAA.png format

# 3. Train
python main.py train --mode kfold --dataset ./data --model yolov8n --epochs 100

# 4. Evaluate multi-view
python -c "
from multiview_inference import MultiViewBatchEvaluator
evaluator = MultiViewBatchEvaluator('./models/best.pt')
results = evaluator.evaluate_test_set('./data/test/images', './data/test/labels')
print(f'Accuracy: {results[\"model_accuracy\"]:.2%}')
"

# 5. Deploy!
```

---

## ğŸ“ Support

For detailed examples and complete implementations:
- See `MULTIVIEW_GUIDE.py` for comprehensive guide
- See `examples.py` for usage examples
- See `QUICK_REFERENCE.md` for command reference

---

## ğŸ“ Version

- **Core System**: v1.0.0
- **Multi-View Extension**: v1.0.0
- **Last Updated**: November 2024
- **Status**: Production Ready âœ…

---

## ğŸ‰ You're Ready!

Your complete detection system with multi-view support is ready to use. Train your model using the standard pipeline, then deploy with multi-view logic for superior accuracy and reliability!

**Happy Building! ğŸš€**
